# SEO Metadata Crawler

A command-line utility written in **[Deno](https://deno.land)** that crawls a website, collects on-page SEO metadata, and produces a consolidated report (`json`, `csv`, or `html`).

---

## âœ¨ Features

* Parses a website's **HTML** and (optionally) **sitemap.xml**
* Extracts common SEO fields (title, meta-description, keywords, H1, canonical, Open Graph, Twitter tagsâ€¦)
* Calculates simple quality scores (e.g. title length, description length, H1 count)
* **Suggests improved title, description, and keywords using OpenAI** (if needed)
* Generates a **summary table** + machine-readable report file
* Handles crawling depth, concurrency limits, timeout, user-agent and more via CLI flags
* Supports output formats: **JSON**, **CSV**, or **HTML**
* Can run in *sitemap-only* mode or follow in-page links
* **Resumes from cache**: will not re-crawl already processed pages
* Uses a `.env` file for your OpenAI API key
* **[NEW] Optional "push-back" add-on** â€“ write the suggested metadata straight into your CMS (AEM today, WordPress/Drupal soon)

---

## ğŸ”§ Prerequisites

1. **Install Deno** (v1.41 or higher recommended):

```bash
# macOS / Linux
curl -fsSL https://deno.land/install.sh | sh

# Windows (PowerShell)
iwr https://deno.land/install.ps1 -useb | iex
```

> Verify installation with `deno --version`.

2. **Set up your OpenAI API key**

Create a `.env` file in your project root:

```
OPENAI_API_KEY=your-key-here
```

---

## ğŸš€ Usage

```bash
deno run --allow-net --allow-read --allow-write --allow-env main.ts --url https://example.com [options]
```

### Common Flags

| Flag | Alias | Default | Description |
| ---- | ----- | ------- | ----------- |
| `--url` | `-u` | â€“ | **Required.** Base URL to crawl |
| `--page` | `-p` | â€“ | Crawl a single page only (disables link following) |
| `--sitemap` | `-s` | `BASE_URL/sitemap.xml` | Explicit sitemap URL |
| `--depth` | `-d` | `3` | Maximum crawl depth |
| `--limit` | `-l` | `100` | Max pages to process |
| `--timeout` | `-t` | `10000` (ms) | Request timeout |
| `--concurrency` | `-c` | `5` | Parallel fetches |
| `--output` | `-o` | `seo-report` | Output file name (extension added automatically) |
| `--format` | `-f` | `json` | `json`, `csv`, or `html` |
| `--sitemap-only` | â€“ | `false` | Only crawl URLs present in sitemap |
| `--follow-links` | â€“ | `true` | Follow in-page links |
| `--user-agent` | â€“ | `SEO-Metadata-Crawler/1.0 (Deno)` | Custom UA string |
| `--force` | `-F` | `false` | (Planned) Ignore cache and force fresh crawl |
| `--push-cms` | `-P` | `false` | Push suggested metadata back into CMS |
| `--cms-config` | `-C` | `cms.json` | Path to CMS JSON config (optional, defaults to cms.json) |
| `--help` | `-h` | â€“ | Show help |

*Use either `--url` (site crawl) **or** `--page` (single-page crawl). If both are supplied, the single-page mode takes precedence.*

### Example

```bash
# Crawl example.com up to depth 2, output CSV
deno run --allow-net --allow-read --allow-write --allow-env main.ts \
  --url https://example.com \
  --depth 2 \
  --format csv
```

After completion you will find `seo-report.csv` (or `.json`/`.html`) in the output directory and a colourful summary printed to the terminal.

### Pushing Updates to AEM

```bash
deno run --allow-net --allow-read --allow-write --allow-env main.ts \
  --url https://www.example.com \
  --push-cms
```

# Or specify a custom config file:
# deno run ... --push-cms --cms-config path/to/your-config.json

`cms.json` example:
```json
{
  "aem": {
    "type": "aem",
    "authorUrl": "https://author.example.com",
    "publishUrl": "https://www.example.com",
    "username": "service-user",
    "password": "â€¢â€¢â€¢â€¢â€¢â€¢",
    "sitePathPrefix": "/content/mysite",
    "replicateAfterUpdate": true,
    "propertyMap": {
      "title": "jcr:title",
      "description": "cq:description",
      "keywords": "cq:keywords"
    }
  }
}
```

Other CMS types (WordPress, Drupal) will get their own schemas in future releases.

The tool will update all fields defined in `propertyMap`. To control which fields are updated, simply adjust the keys in `propertyMap`.

---

## ğŸ— Building a Native Executable

Deno can bundle this script (including its dependencies) into a single binary.
Use the provided **Deno task** or run the command directly:

```bash
# via deno task (requires deno.json â€“ see below)
deno task build

# or directly
deno compile \
  --allow-net --allow-read --allow-write --allow-env \
  --output seo-crawler \
  main.ts
```

This creates a platform-specific executable named `seo-crawler` (or `.exe` on Windows) that you can distribute without requiring Deno.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ main.ts         # Main CLI entry point
â”œâ”€â”€ types.ts        # Type definitions
â”œâ”€â”€ crawl.ts        # Crawl logic
â”œâ”€â”€ metadata.ts     # Metadata extraction
â”œâ”€â”€ openai.ts       # OpenAI integration
â”œâ”€â”€ report.ts       # Report generation
â”œâ”€â”€ robots.ts       # robots.txt logic
â”œâ”€â”€ sitemap.ts      # Sitemap parsing
â”œâ”€â”€ utils/          # Utility modules (url, file, csv, html)
â”œâ”€â”€ output/         # Output and cache files
â”œâ”€â”€ .env            # Your OpenAI API key
â””â”€â”€ README.md       # This file
```
